# Agnos DevOps Assignment

A production-ready DevOps setup with two independent Go microservices, CloudNativePG HA database, Docker containerization, Kubernetes orchestration across 3 environments (DEV/UAT/PROD), and a **3-branch CI/CD pipeline** (`dev` â†’ `uat` â†’ `prod`) with environment isolation and manual approval gates.

---

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Kind Kubernetes Cluster                       â”‚
â”‚              (1 control-plane + 3 worker nodes)                  â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  agnos-dev   â”‚  â”‚  agnos-uat   â”‚  â”‚  agnos-prod  â”‚             â”‚
â”‚  â”‚  1 API       â”‚  â”‚  3 API       â”‚  â”‚  3 API       â”‚             â”‚
â”‚  â”‚  1 Worker    â”‚  â”‚  2 Worker    â”‚  â”‚  2 Worker     â”‚             â”‚
â”‚  â”‚  2 Postgres  â”‚  â”‚  3 Postgres  â”‚  â”‚  3 Postgres   â”‚             â”‚
â”‚  â”‚  (CNPG HA)   â”‚  â”‚  (CNPG HA)   â”‚  â”‚  (CNPG HA)    â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                                   â”‚
â”‚  Each namespace contains:                                         â”‚
â”‚  ConfigMap, Secret, Deployments, Services, HPA, PDB,             â”‚
â”‚  NetworkPolicy, Ingress, LimitRange, ResourceQuota               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Services

| Service | Description | Port | Endpoints |
|---------|-------------|------|-----------|
| **API** | HTTP API server with health checks and Prometheus metrics | 8080 (internal), 8090 (public) | `GET /live`, `GET /ready`, `GET /metrics`, `GET /api/v1/time` (public) |
| **Worker** | Background service that periodically processes batches from PostgreSQL | 8081 | `GET /live`, `GET /ready`, `GET /metrics` |
| **PostgreSQL** | CloudNativePG HA cluster with streaming replication and auto-failover | 5432 | â€” |

---

## Project Structure

Each service is a **fully independent microservice** with its own `go.mod`, dependencies, and Dockerfile.

```
.
â”œâ”€â”€ api/                              # API microservice
â”‚   â”œâ”€â”€ main.go                       # Source code
â”‚   â”œâ”€â”€ main_test.go                  # Unit tests
â”‚   â”œâ”€â”€ Dockerfile                    # Multi-stage Docker build
â”‚   â”œâ”€â”€ .dockerignore
â”‚   â”œâ”€â”€ go.mod                        # Independent Go module
â”‚   â””â”€â”€ go.sum
â”œâ”€â”€ worker/                           # Worker microservice
â”‚   â”œâ”€â”€ main.go                       # Source code (with Prometheus metrics)
â”‚   â”œâ”€â”€ main_test.go                  # Unit tests
â”‚   â”œâ”€â”€ Dockerfile                    # Multi-stage Docker build
â”‚   â”œâ”€â”€ .dockerignore
â”‚   â”œâ”€â”€ go.mod
â”‚   â””â”€â”€ go.sum
â”œâ”€â”€ k8s/                              # Kubernetes manifests
â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”œâ”€â”€ namespaces.yaml           # agnos-dev, agnos-uat, agnos-prod
â”‚   â”‚   â””â”€â”€ network-policy.yaml       # Network policies
â”‚   â”œâ”€â”€ envs/
â”‚   â”‚   â”œâ”€â”€ dev/all.yaml              # DEV: all resources in one file
â”‚   â”‚   â”œâ”€â”€ uat/all.yaml              # UAT: all resources in one file
â”‚   â”‚   â””â”€â”€ prod/all.yaml             # PROD: all resources in one file
â”‚   â”œâ”€â”€ chaos/                        # Chaos Mesh experiments (manual)
â”‚   â”‚   â”œâ”€â”€ pod-failure.yaml          # Pod kill experiment
â”‚   â”‚   â”œâ”€â”€ pod-stress.yaml           # CPU/memory stress
â”‚   â”‚   â””â”€â”€ network-delay.yaml        # Network latency injection
â”‚   â”œâ”€â”€ litmus/                       # LitmusChaos experiments (CI/CD)
â”‚   â”‚   â”œâ”€â”€ dev/                      # DEV chaos experiments
â”‚   â”‚   â”‚   â”œâ”€â”€ rbac.yaml             # ServiceAccount & RBAC
â”‚   â”‚   â”‚   â”œâ”€â”€ pod-delete.yaml       # Pod delete experiment
â”‚   â”‚   â”‚   â”œâ”€â”€ pod-network-latency.yaml
â”‚   â”‚   â”‚   â””â”€â”€ pod-cpu-hog.yaml
â”‚   â”‚   â””â”€â”€ uat/                      # UAT chaos experiments (longer durations)
â”‚   â”‚       â”œâ”€â”€ rbac.yaml
â”‚   â”‚       â”œâ”€â”€ pod-delete.yaml
â”‚   â”‚       â”œâ”€â”€ pod-network-latency.yaml
â”‚   â”‚       â””â”€â”€ pod-cpu-hog.yaml
â”‚   â””â”€â”€ prometheus/
â”‚       â”œâ”€â”€ prometheus-config.yaml
â”‚       â””â”€â”€ alert-rules.yaml
â”œâ”€â”€ k6/                               # Load testing
â”‚   â”œâ”€â”€ load-test.js                  # Normal load test (50 VUs)
â”‚   â””â”€â”€ stress-test.js                # Stress test (200 VUs)
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ ci-cd.yaml                    # 3-branch CI/CD pipeline (dev/uat/prod)
â”œâ”€â”€ docker-compose.yaml               # Local dev orchestration
â”œâ”€â”€ docker-compose.sonarqube.yaml     # SonarQube for code quality
â”œâ”€â”€ sonar-project.properties          # SonarQube config
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run-local.sh                  # Run CI/CD locally with act
â”œâ”€â”€ .actrc                            # act CLI config
â””â”€â”€ .github/act.env                   # act environment variables
```

---

## CI/CD Pipeline

The CI/CD pipeline uses a **3-branch strategy** where each branch deploys exclusively to its own environment:

### Branching Strategy

```
dev â”€â”€PRâ”€â”€â–¶ uat â”€â”€PRâ”€â”€â–¶ prod
 â”‚           â”‚            â”‚
 â–¼           â–¼            â–¼
DEV only    UAT only    PROD only
```

| Branch | Trigger | Deploys To | Key Stages |
|--------|---------|-----------|------------|
| `dev` | push | DEV only | Lint, Test, SAST, Build, Scan, Deploy DEV |
| `uat` | push | UAT (pre-prod) | Lint, Test, SAST, SonarQube, Build, Scan, Deploy UAT, Chaos, Load Test |
| `prod` | push | PROD only | Lint, Test, SAST, SonarQube, Build, Scan, Deploy PROD (manual approval) |
| PR to `uat`/`prod` | pull_request | Nothing | Lint, Test, SAST (validation only) |

### Image Tagging

Images are tagged with environment + short SHA for traceability:
- `dev` branch â†’ `agnos/api:dev-abc1234` + `agnos/api:latest`
- `uat` branch â†’ `agnos/api:uat-abc1234` + `agnos/api:latest`
- `prod` branch â†’ `agnos/api:prod-abc1234` + `agnos/api:latest`

### Pipeline Stages

```
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â”‚                        dev branch                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚ 1. Lint & â”‚â”€â”€â–¶â”‚ 2. SAST   â”‚â”€â”€â–¶â”‚  â”‚ 3. Docker â”‚â”€â”€â–¶â”‚ 4. Image  â”‚â”€â”€â–¶â”‚ 5. Deploy â”‚                   â”‚
â”‚    Test   â”‚   â”‚  (gosec)  â”‚   â”‚  â”‚   Build   â”‚   â”‚   Scan    â”‚   â”‚    DEV    â”‚                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â”‚                              uat branch                                         â”‚
                                â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                                â”‚  â”‚ 3. Sonar- â”‚â”€â”€â–¶â”‚ 4. Docker â”‚â”€â”€â–¶â”‚ 5. Image  â”‚â”€â”€â–¶â”‚ 6. Deploy â”‚â”€â”€â–¶â”‚ 7. Chaos  â”‚  â”‚
                                â”‚  â”‚    Qube   â”‚   â”‚   Build   â”‚   â”‚   Scan    â”‚   â”‚    UAT    â”‚   â”‚  + k6     â”‚  â”‚
                                â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â”‚                         prod branch                            â”‚
                                â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                                â”‚  â”‚ 3. Sonar- â”‚â”€â”€â–¶â”‚ 4. Docker â”‚â”€â”€â–¶â”‚ 5. Image  â”‚â”€â”€â–¶â”‚ 6. Deploy â”‚ â”‚
                                â”‚  â”‚    Qube   â”‚   â”‚   Build   â”‚   â”‚   Scan    â”‚   â”‚   PROD ğŸ”’ â”‚ â”‚
                                â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

1. **Lint & Test** â€” golangci-lint + `go test` with race detection and coverage (matrix: api, worker)
2. **SAST** â€” gosec security scan on both services
3. **SonarQube** â€” Code quality analysis + Quality Gate check (uat + prod only)
4. **Build** â€” Multi-stage Docker image builds, tagged with `ENV-SHA` (push only)
5. **Image Scan** â€” Trivy vulnerability and secret scanning
6. **Deploy** â€” Each branch deploys only to its own environment with E2E health verification
7. **Chaos Test** â€” LitmusChaos pod-delete experiments (uat only)
8. **Load Test** â€” k6 load test against Docker Compose services (uat only)

### Deployment Prerequisites

Each deploy job automatically installs the required infrastructure:
- **Kind** â€” Local Kubernetes cluster (1 control-plane + 2-3 workers)
- **kubectl** â€” Kubernetes CLI
- **CloudNativePG operator** â€” HA PostgreSQL via `Cluster` CRD

### Run Locally with act

```bash
# Prerequisites: act, Docker (recommended: 4+ GB RAM, 4+ CPUs in Docker Desktop)
brew install act

# Run full pipeline (default: uses current branch)
./scripts/run-local.sh

# Run a specific job
./scripts/run-local.sh lint-and-test
./scripts/run-local.sh build
./scripts/run-local.sh deploy-dev
```

### Run act for Each Environment

Each branch triggers a different pipeline. Use event files to simulate pushes to specific branches:

```bash
# DEV pipeline: lint â†’ sast â†’ build â†’ image-scan â†’ deploy-dev
act push --eventpath .github/events/push-dev.json --env-file .github/act.env --privileged

# UAT pipeline: lint â†’ sast â†’ sonarqube â†’ build â†’ image-scan â†’ deploy-uat â†’ chaos-test + load-test
act push --eventpath .github/events/push-uat.json --env-file .github/act.env --privileged

# PROD pipeline: lint â†’ sast â†’ sonarqube â†’ build â†’ image-scan â†’ deploy-prod
act push --eventpath .github/events/push-prod.json --env-file .github/act.env --privileged

# Run a specific job only
act push --eventpath .github/events/push-dev.json --env-file .github/act.env --privileged -j deploy-dev
```

**Note:** SonarQube must be running for UAT/PROD pipelines:
```bash
docker compose -f docker-compose.sonarqube.yaml up -d
```

### Deploy All Environments Manually (without act)

```bash
# 1. Build images
docker build -t agnos/api:latest ./api
docker build -t agnos/worker:latest ./worker

# 2. Create Kind cluster
kind create cluster --name agnos-cluster --config=- <<'EOF'
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
    extraPortMappings:
      - containerPort: 30090
        hostPort: 9090
      - containerPort: 30091
        hostPort: 9091
      - containerPort: 30092
        hostPort: 9092
  - role: worker
  - role: worker
  - role: worker
EOF

# 3. Load images and install CNPG
kind load docker-image agnos/api:latest agnos/worker:latest --name agnos-cluster
kubectl apply --server-side -f https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/release-1.25/releases/cnpg-1.25.1.yaml
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=cloudnative-pg -n cnpg-system --timeout=120s

# 4. Deploy all environments
kubectl apply -f k8s/base/namespaces.yaml
kubectl apply -f k8s/envs/dev/all.yaml
kubectl apply -f k8s/envs/uat/all.yaml
kubectl apply -f k8s/envs/prod/all.yaml

# 5. Wait for pods
kubectl get pods -A -w
```

---

## Local Development

### Run with Docker Compose

```bash
docker compose up --build
```

This starts PostgreSQL, API (port 8080), and Worker (port 8081) with health checks.

### Run Services Directly

```bash
# API (terminal 1)
cd api && APP_ENV=development DB_DSN="postgres://agnos_dev:agnos_dev_pass@localhost:5433/agnos_dev?sslmode=disable" go run .

# Worker (terminal 2)
cd worker && APP_ENV=development WORKER_INTERVAL=10s DB_DSN="postgres://agnos_dev:agnos_dev_pass@localhost:5433/agnos_dev?sslmode=disable" go run .
```

### Accessing the Local Database

**1. If running with Docker Compose:**
The database is mapped to host port `5433` to prevent conflicts with local instances.
- **URI**: `postgres://agnos_dev:agnos_dev_pass@localhost:5433/agnos_dev?sslmode=disable`
- **Host**: `localhost` | **Port**: `5433` | **DB**: `agnos_dev` | **User**: `agnos_dev` | **Password**: `agnos_dev_pass`

**2. If running via `act` (Kind cluster):**
The Kind cluster remains active when using `.github/act.env` (`SKIP_KIND_CLEANUP=true`). You can port-forward the CloudNativePG `postgres-rw` service to your host:

```bash
# Port-forward the DEV database to local port 5432
kubectl port-forward -n agnos-dev svc/postgres-rw 5432:5432
```
After port-forwarding, connect via:
- **URI**: `postgres://agnos_dev:agnos_dev_pass@localhost:5432/agnos_dev?sslmode=disable`
- **Host**: `localhost` | **Port**: `5432` | **DB**: `agnos_dev` | **User**: `agnos_dev` | **Password**: `agnos_dev_pass`


### Verify

```bash
# Liveness
curl http://localhost:8080/live

# Readiness
curl http://localhost:8080/ready

# Prometheus metrics (API)
curl http://localhost:8080/metrics

# Prometheus metrics (Worker)
curl http://localhost:8081/metrics
```

### Public API

The API exposes a dedicated public endpoint on a separate port (`PUBLIC_PORT`, default `8090`). This endpoint is the **only** externally accessible route via NodePort; internal endpoints (`/live`, `/ready`, `/metrics`) remain cluster-internal on port 8080.

```bash
# Public time endpoint
curl http://localhost:8090/api/v1/time
# {"status":"ok","timestamp":"2026-02-27T12:00:00Z","env":"development"}
```

**Per-environment NodePort access (Kind cluster):**

| Env  | NodePort | Host Port | URL |
|------|----------|-----------|-----|
| DEV  | 30090    | 9090      | `http://localhost:9090/api/v1/time` |
| UAT  | 30091    | 9091      | `http://localhost:9091/api/v1/time` |
| PROD | 30092    | 9092      | `http://localhost:9092/api/v1/time` |

**Network policy design:**

```
External traffic --> NodePort 300xx --> port 8090 --> /api/v1/time ONLY (public server)
Cluster internal --> ClusterIP :80  --> port 8080 --> /live, /ready, /metrics (internal server)
```

---

## How to Run Tests

```bash
# API tests
cd api && go test -v -race ./...

# Worker tests
cd worker && go test -v -race ./...

# With coverage
cd api && go test -race -coverprofile=coverage.out ./... && go tool cover -func=coverage.out

# Automated Test Result Collection
# Gather unit tests, SAST reports, SonarQube metrics, and load/stress test
# output into the `results/` folder (runs locally or leverages act CI cache)
./scripts/collect-results.sh
```

---

## Load Testing

```bash
# Install k6
brew install k6

# Normal load test (50 VUs, 5 min)
k6 run k6/load-test.js

# Stress test (200 VUs, 8 min)
k6 run k6/stress-test.js

# Against a specific endpoint
k6 run --env BASE_URL=http://api-uat.agnos.local k6/load-test.js
```

---

## Docker

### Build Images

```bash
docker build -t agnos/api:latest ./api
docker build -t agnos/worker:latest ./worker
```

### Multi-stage Build

Both Dockerfiles use a multi-stage build:
1. **Builder** (`golang:1.25-alpine`): Compiles with `-ldflags="-w -s"` for minimal binary
2. **Runtime** (`gcr.io/distroless/static:nonroot`): Minimal attack surface, non-root user

### Environment Variables

| Variable | Default | Used By | Description |
|----------|---------|---------|-------------|
| `APP_ENV` | `development` | Both | Environment name |
| `APP_VERSION` | `1.0.0` | API | Version in health response |
| `PORT` | `8080` | API | Internal API listen port |
| `PUBLIC_PORT` | `8090` | API | Public API listen port |
| `HEALTH_PORT` | `8081` | Worker | Worker health port |
| `WORKER_INTERVAL` | `60s` | Worker | Job interval |
| `DB_DSN` | â€” | Both | PostgreSQL connection string |
| `RATE_LIMIT` | `100` | API | Requests per second limit |

---

## Kubernetes

### Deploy to Kind

Each branch deploys only its own environment to a Kind cluster. The CI/CD pipeline automatically installs Kind, kubectl, and the CloudNativePG operator before deploying.

UAT is configured as a **pre-prod** environment â€” identical resource/replica/HA config as PROD so that chaos tests and load tests accurately simulate production behavior.

| Environment | Namespace | API Replicas | Worker Replicas | Postgres Instances (CNPG) | Notes |
|-------------|-----------|-------------|----------------|--------------------------|-------|
| DEV | `agnos-dev` | 1 | 1 | 2 (1 primary + 1 replica) | Lightweight for fast iteration |
| UAT (pre-prod) | `agnos-uat` | 3 | 2 | 3 (1 primary + 2 replicas) | Mirrors PROD config |
| PROD | `agnos-prod` | 3 | 2 | 3 (1 primary + 2 replicas) | Manual approval required |

### Manual Deploy

```bash
# Install CloudNativePG operator first (required for PostgreSQL HA clusters)
kubectl apply --server-side -f https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/release-1.25/releases/cnpg-1.25.1.yaml
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=cloudnative-pg -n cnpg-system --timeout=120s

# Create namespaces and deploy environments
kubectl apply -f k8s/base/namespaces.yaml
kubectl apply -f k8s/envs/dev/all.yaml
kubectl apply -f k8s/envs/uat/all.yaml
kubectl apply -f k8s/envs/prod/all.yaml
```

### Connect to Kind Cluster (kubectl / Lens)

After the CI/CD pipeline runs with `SKIP_KIND_CLEANUP=true`, the Kind cluster remains available. To connect:

```bash
# List available Kind clusters
kind get clusters

# Set kubectl context to the Kind cluster
kubectl cluster-info --context kind-agnos-cluster

# If the context is not set automatically, export the kubeconfig
kind export kubeconfig --name agnos-cluster

# Verify connection
kubectl get nodes
kubectl get pods -A

# View resources per environment
kubectl get all -n agnos-dev
kubectl get all -n agnos-uat
kubectl get all -n agnos-prod
```

To use with **Lens**:
1. Run `kind export kubeconfig --name agnos-cluster` to ensure the context is in `~/.kube/config`
2. Open Lens and it will auto-detect the `kind-agnos-cluster` context
3. Or manually add the cluster via File > Add Cluster and paste the output of `kind get kubeconfig --name agnos-cluster`

### Clean Up Kind Cluster

```bash
# Delete the Kind cluster (removes all deployed environments)
kind delete cluster --name agnos-cluster

# If using a separate chaos testing cluster
kind delete cluster --name agnos-chaos

# Delete all Kind clusters
kind delete clusters --all
```

### CloudNativePG (HA PostgreSQL)

Each environment uses a CloudNativePG `Cluster` CRD instead of a single Postgres Deployment:
- **Streaming replication** with automatic failover
- **Self-healing**: failed instances are automatically replaced
- **`postgres-rw`** service: always points to the primary (read-write)
- **`postgres-ro`** service: load-balanced across replicas (read-only)
- **PodMonitor** for Prometheus metrics

### High Availability Features

- **Multiple replicas** with pod anti-affinity across nodes
- **HPA**: Auto-scaling based on CPU (70%) / memory (80%)
- **PodDisruptionBudget**: Minimum availability during disruptions
- **Rolling updates**: `maxUnavailable: 0`, `maxSurge: 1`
- **Readiness/liveness probes** on both services
- **Resource requests/limits** to prevent noisy neighbors
- **ResourceQuota / LimitRange** per namespace
- **NetworkPolicy** for all components (API, Worker, Postgres)
- **Ingress** for UAT/PROD external access
- **Graceful shutdown** period

### Security Notes

Secrets in `k8s/envs/*/all.yaml` contain **placeholder values** for local/CI usage only. In a real production environment, use one of:
- [Sealed Secrets](https://github.com/bitnami-labs/sealed-secrets)
- [External Secrets Operator](https://external-secrets.io/)
- [SOPS](https://github.com/getsops/sops) with age/KMS encryption

---

## Monitoring

### Structured JSON Logs

```json
{"time":"2026-02-25T14:00:00Z","level":"INFO","msg":"request completed","method":"GET","path":"/live","status":200,"duration_ms":0.5}
```

### Prometheus Metrics

**API Metrics:**

| Metric | Type | Description |
|--------|------|-------------|
| `http_requests_total` | Counter | Requests by method/endpoint/status |
| `http_request_duration_seconds` | Histogram | Latency distribution |
| `http_errors_total` | Counter | 4xx/5xx errors |
| `http_rate_limited_total` | Counter | Rate-limited requests |

**Worker Metrics:**

| Metric | Type | Description |
|--------|------|-------------|
| `worker_logs_processed_total` | Counter | Total log entries processed |
| `worker_processing_duration_seconds` | Histogram | Batch processing duration |
| `worker_batch_errors_total` | Counter | Batch processing errors |

### Alert Rules (`k8s/prometheus/alert-rules.yaml`)

| Alert | Condition | Severity |
|-------|-----------|----------|
| HighErrorRate | >5% errors for 5 min | Critical |
| HighRequestLatency | p95 >1s for 5 min | Warning |
| WorkerStalled | Unhealthy for 5 min | Critical |
| PodCrashLooping | Frequent restarts | Critical |
| PodNotReady | Not ready for 5 min | Warning |
| HighDBLatency | DB p95 >500ms for 5 min | Warning |
| HighMemoryUsage | >80% memory limit for 5 min | Warning |
| APIDown | All replicas down for 1 min | Critical |

---

## Chaos Engineering

### LitmusChaos (CI/CD Integrated)

LitmusChaos experiments in `k8s/litmus/` run automatically in the CI/CD pipeline for UAT:

| Experiment | UAT | Description |
|-----------|-----|-------------|
| `pod-delete` | 60s, 50% pods | Random pod kill to validate auto-recovery |
| `pod-network-latency` | 120s, 300ms | Network latency injection |
| `pod-cpu-hog` | 120s, 1 core | CPU stress to validate HPA scaling |

```bash
# Manual usage
kubectl apply -f k8s/litmus/uat/rbac.yaml
kubectl apply -f k8s/litmus/uat/pod-delete.yaml
kubectl get chaosresult -n agnos-uat
```

### Chaos Mesh (Manual)

Additional Chaos Mesh experiments in `k8s/chaos/` for ad-hoc resilience testing:

| Experiment | Target | Description |
|-----------|--------|-------------|
| `pod-failure.yaml` | API pods | Random pod kill every 10 min |
| `pod-stress.yaml` | API pods | CPU (80%) + memory (256MB) stress for 5 min |
| `network-delay.yaml` | API â†’ Postgres | 200ms latency injection for 5 min |

---

## Failure Scenario Handling

### 1. API crashes during peak hours

**Detection:** Liveness probe fails â†’ K8s restarts pod. HPA detects high CPU â†’ scales up.

**Mitigation:** Multiple replicas + anti-affinity ensure availability. HPA auto-scales for load spikes. HighErrorRate alert notifies team.

**Recovery:** K8s auto-restarts crashed pods. Load balancer routes to healthy pods. Investigate via structured logs.

### 2. Worker fails and infinitely retries

**Detection:** Health endpoint returns 503 â†’ liveness probe fails â†’ K8s restarts.

**Mitigation:** Staleness detection (unhealthy if no job runs within 3x interval). WorkerStalled alert fires after 5 min. Resource limits prevent runaway consumption.

**Recovery:** K8s restarts after liveness failure. Add exponential backoff + max retry limit + dead-letter queue for persistent issues.

### 3. Bad deployment is released

**Detection:** Readiness probe fails on new pods â†’ no traffic routed.

**Mitigation:** Rolling update with `maxUnavailable: 0` keeps old pods serving. CI/CD runs lint, tests, SAST, and Trivy before deploy. Code must pass through `dev` â†’ `uat` â†’ `prod` branches via PRs. PROD requires manual approval via GitHub Environment protection rules.

**Recovery:**
```bash
kubectl rollout undo deployment/api -n agnos-prod
```

### 4. Kubernetes node goes down

**Detection:** K8s marks pods as Terminating.

**Mitigation:** Pod anti-affinity spreads replicas across nodes. ReplicaSet reschedules to healthy nodes. PodDisruptionBudget ensures minimum availability.

**Recovery:** Automatic â€” K8s reschedules pods. No manual intervention for stateless services.

### 5. PostgreSQL primary fails

**Detection:** CloudNativePG detects primary failure via health checks.

**Mitigation:** CloudNativePG automatically promotes a replica to primary. `postgres-rw` service seamlessly points to the new primary.

**Recovery:** Automatic â€” CNPG self-heals by recreating the failed instance as a new replica.

---

## Tech Stack

| Component | Technology |
|-----------|------------|
| Language | Go 1.25 |
| Database | PostgreSQL 15 (CloudNativePG HA) |
| Container | Docker (multi-stage, distroless) |
| Orchestration | Kubernetes (Kind) |
| CI/CD | GitHub Actions (3-branch strategy: dev/uat/prod) |
| Linting | golangci-lint |
| SAST | gosec |
| Code Quality | SonarQube + Quality Gate |
| Image Scan | Trivy |
| Load Testing | k6 |
| Chaos Engineering | LitmusChaos (CI/CD) + Chaos Mesh (manual) |
| Monitoring | Prometheus |
| Logging | `log/slog` (structured JSON) |
